{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "logreg_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8UYyAG6G2w1",
        "outputId": "0f3da54c-4649-4654-96bc-6603014a0f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqdjVCWEG2w5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vl6I9wdMJPZ"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from matplotlib.pyplot import imread\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "\n",
        "def load_notmnist(path='./notMNIST_small',letters='ABCDEFGHIJ',\n",
        "                  img_shape=(28,28),test_size=0.25,one_hot=False):\n",
        "    \n",
        "    # download data if it's missing. If you have any problems, go to the urls and load it manually.\n",
        "    if not os.path.exists(path):\n",
        "        if not os.path.exists('./notMNIST_small.tar.gz'):\n",
        "            print(\"Downloading data...\")\n",
        "            assert os.system('curl http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz > notMNIST_small.tar.gz') == 0\n",
        "        print(\"Extracting ...\")\n",
        "        assert os.system('tar -zxvf notMNIST_small.tar.gz > untar_notmnist.log') == 0\n",
        "    \n",
        "    data,labels = [],[]\n",
        "    print(\"Parsing...\")\n",
        "    for img_path in glob(os.path.join(path,'*/*')):\n",
        "        class_i = img_path.split(os.sep)[-2]\n",
        "        if class_i not in letters: continue\n",
        "        try:\n",
        "            data.append(imread(img_path))\n",
        "            labels.append(class_i,)\n",
        "        except:\n",
        "            print(\"found broken img: %s [it's ok if <10 images are broken]\" % img_path)\n",
        "        \n",
        "    data = np.stack(data)[:,None].astype('float32')\n",
        "    data = (data - np.mean(data)) / np.std(data)\n",
        "\n",
        "    #convert classes to ints\n",
        "    letter_to_i = {l:i for i,l in enumerate(letters)}\n",
        "    labels = np.array(list(map(letter_to_i.get, labels)))\n",
        "    \n",
        "    if one_hot:\n",
        "        labels = (np.arange(np.max(labels) + 1)[None,:] == labels[:, None]).astype('float32')\n",
        "    \n",
        "    #split into train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=test_size, random_state=42)\n",
        "    \n",
        "    print(\"Done\")\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Ag6Y9uG2xp"
      },
      "source": [
        "# High-level pytorch\n",
        "\n",
        "So far we've been dealing with low-level torch API. While it's absolutely vital for any custom losses or layers, building large neura nets in it is a bit clumsy.\n",
        "\n",
        "Luckily, there's also a high-level torch interface with a pre-defined layers, activations and training algorithms. \n",
        "\n",
        "We'll cover them as we go through a simple image recognition problem: classifying letters into __\"A\"__ vs __\"B\"__.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhB_OEQtG2xp",
        "outputId": "96593cff-1b8c-48d3-ae2f-a5e540228f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "X_train, y_train, X_test, y_test = load_notmnist(letters='AB')\n",
        "X_train, X_test = X_train.reshape([-1, 784]), X_test.reshape([-1, 784])\n",
        "\n",
        "print(\"Train size = %i, test_size = %i\"%(len(X_train),len(X_test)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data...\n",
            "Extracting ...\n",
            "Parsing...\n",
            "found broken img: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png [it's ok if <10 images are broken]\n",
            "Done\n",
            "Train size = 2808, test_size = 937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DL0n6aUdG2xt",
        "outputId": "0c5cf53e-a5b5-45fd-dc01-60fe8b674738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "for i in [0,1]:\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.imshow(X_train[i].reshape([28,28]))\n",
        "    plt.title(str(y_train[i]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lXEo7ZiG2xw"
      },
      "source": [
        "Let's start with layers. The main abstraction here is __`torch.nn.Module`__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXCDLOECG2xw",
        "outputId": "1b114e21-1c98-4908-9087-b8675468bd11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(nn.Module.__doc__)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base class for all neural network modules.\n",
            "\n",
            "    Your models should also subclass this class.\n",
            "\n",
            "    Modules can also contain other Modules, allowing to nest them in\n",
            "    a tree structure. You can assign the submodules as regular attributes::\n",
            "\n",
            "        import torch.nn as nn\n",
            "        import torch.nn.functional as F\n",
            "\n",
            "        class Model(nn.Module):\n",
            "            def __init__(self):\n",
            "                super(Model, self).__init__()\n",
            "                self.conv1 = nn.Conv2d(1, 20, 5)\n",
            "                self.conv2 = nn.Conv2d(20, 20, 5)\n",
            "\n",
            "            def forward(self, x):\n",
            "                x = F.relu(self.conv1(x))\n",
            "                return F.relu(self.conv2(x))\n",
            "\n",
            "    Submodules assigned in this way will be registered, and will have their\n",
            "    parameters converted too when you call :meth:`to`, etc.\n",
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVBB3Z1OG2x0"
      },
      "source": [
        "There's a vast library of popular layers and architectures already built for ya'.\n",
        "\n",
        "This is a binary classification problem, so we'll train a __Logistic Regression with sigmoid__.\n",
        "$$P(y_i | X_i) = \\sigma(W \\cdot X_i + b) ={ 1 \\over {1+e^{- [W \\cdot X_i + b]}} }$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhnvVPJeG2x0"
      },
      "source": [
        "# create a network that stacks layers on top of each other\n",
        "model = nn.Sequential()\n",
        "\n",
        "# add first \"dense\" layer with 784 input units and 1 output unit. \n",
        "model.add_module('l1', nn.Linear(784, 1))\n",
        "\n",
        "# add softmax activation for probabilities. Normalize over axis 1\n",
        "# note: layer names must be unique\n",
        "model.add_module('l2', nn.Sigmoid())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxoZgD8-G2x3",
        "outputId": "674b5b0b-1ff8-4c81-e46a-9e3e9cb0a9be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Weight shapes:\", [w.shape for w in model.parameters()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weight shapes: [torch.Size([1, 784]), torch.Size([1])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMv-3WO5QCK2"
      },
      "source": [
        "#  выведите обучаемые параметры\n",
        "# ваш код здесь"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFhmv06dG2x6",
        "outputId": "133a0371-d7ab-4ae8-9272-16b1c755bf12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# create dummy data with 3 samples and 784 features\n",
        "\n",
        "# ваш код здесь\n",
        "\n",
        "# compute outputs given inputs, both are variables\n",
        "\n",
        "# ваш код здесь\n",
        "\n",
        "# display what we've got\n",
        "# ваш код здесь"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7171, 0.4256, 0.5188], grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2HX1co0G2x9"
      },
      "source": [
        "Let's now define a loss function for our model.\n",
        "\n",
        "The natural choice is to use binary crossentropy (aka logloss, negative llh):\n",
        "$$ L = {1 \\over N} \\underset{X_i,y_i} \\sum - [  y_i \\cdot log P(y_i | X_i) + (1-y_i) \\cdot log (1-P(y_i | X_i)) ]$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4VCWuRhG2x-"
      },
      "source": [
        "crossentropy = # ваш код здесь\n",
        "\n",
        "loss = # ваш код здесь\n",
        "\n",
        "assert tuple(crossentropy.size()) == (3,), \"Crossentropy must be a vector with element per sample\"\n",
        "assert tuple(loss.size()) == (1,), \"Loss must be scalar. Did you forget the mean/sum?\"\n",
        "assert crossentropy.data.numpy()[0] > 0, \"Crossentropy must non-negative, zero only for perfect prediction\"\n",
        "assert loss.data.numpy()[0] <= np.log(3), \"Loss is too large even for untrained model. Please double-check it.\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gl4PYN7G2yB"
      },
      "source": [
        "__Note:__ you can also find many such functions in `torch.nn.functional`, just type __`F.<tab>`__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QOuYQsDG2yC"
      },
      "source": [
        "__Torch optimizers__\n",
        "\n",
        "When we trained Linear Regression above, we had to manually .zero_() gradients on both our variables. Imagine that code for a 50-layer network.\n",
        "\n",
        "Again, to keep it from getting dirty, there's `torch.optim` module with pre-implemented algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JRbRJJhG2yC"
      },
      "source": [
        "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# here's how it's used:\n",
        "loss.backward()      # add new gradients\n",
        "opt.step()           # change weights\n",
        "opt.zero_grad()      # clear gradients"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyMIM5NeG2yH",
        "outputId": "da2d9da1-3664-4dc7-c37f-e7e5cb3a64db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "# dispose of old variables to avoid bugs later\n",
        "del x, y, y_predicted, loss"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-930a29713988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dispose of old variables to avoid bugs later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mzqecu5G2yK"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exg8MRvJG2yK"
      },
      "source": [
        "# create network again just in case\n",
        "model = nn.Sequential()\n",
        "model.add_module('first', nn.Linear(784, 1))\n",
        "model.add_module('second', nn.Sigmoid())\n",
        "\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4qQetPfG2yN",
        "outputId": "747b593a-dd95-457b-cead-63c282a227e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "history = []\n",
        "\n",
        "for i in range(100):\n",
        "    \n",
        "    # sample 256 random images\n",
        "    # ваш код здесь\n",
        "    \n",
        "    # predict probabilities\n",
        "    # ваш код здесь\n",
        "    \n",
        "    assert y_predicted.dim() == 1, \"did you forget to select first column with [:, 0]\"\n",
        "    \n",
        "    # compute loss, just like before\n",
        "    loss = # ваш код здесь\n",
        "    \n",
        "    # compute gradients\n",
        "    # ваш код здесь\n",
        "    \n",
        "    # Adam step\n",
        "    # ваш код здесь\n",
        "    \n",
        "    # clear gradients\n",
        "    # ваш код здесь\n",
        "    \n",
        "    history.append(loss.data.numpy())\n",
        "    \n",
        "    if i % 10 == 0:\n",
        "        print(\"step #%i | mean loss = %.3f\" % (i, np.mean(history[-10:])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step #0 | mean loss = 0.539\n",
            "step #10 | mean loss = 0.320\n",
            "step #20 | mean loss = 0.212\n",
            "step #30 | mean loss = 0.145\n",
            "step #40 | mean loss = 0.136\n",
            "step #50 | mean loss = 0.128\n",
            "step #60 | mean loss = 0.112\n",
            "step #70 | mean loss = 0.108\n",
            "step #80 | mean loss = 0.103\n",
            "step #90 | mean loss = 0.104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRqCbDVxG2yP"
      },
      "source": [
        "__Debugging tips:__\n",
        "* make sure your model predicts probabilities correctly. Just print them and see what's inside.\n",
        "* don't forget _minus_ sign in the loss function! It's a mistake 99% ppl do at some point.\n",
        "* make sure you zero-out gradients after each step. Srsly:)\n",
        "* In general, pytorch's error messages are quite helpful, read 'em before you google 'em.\n",
        "* if you see nan/inf, print what happens at each iteration to find our where exactly it occurs.\n",
        "  * If loss goes down and then turns nan midway through, try smaller learning rate. (Our current loss formula is unstable).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9vCiuZZG2yP"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Let's see how our model performs on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDjKdpt2G2yQ",
        "outputId": "d854b907-281e-4596-f9f6-c76623f877fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# use your model to predict classes (0 or 1) for all test samples\n",
        "predicted_y_test = # ваш код здесь\n",
        "predicted_y_test = # ваш код здесь\n",
        "         \n",
        "assert isinstance(predicted_y_test, np.ndarray), \"please return np array, not %s\" % type(predicted_y_test)\n",
        "assert predicted_y_test.shape == y_test.shape, \"please predict one class for each test sample\"\n",
        "assert np.in1d(predicted_y_test, y_test).all(), \"please predict class indexes\"\n",
        "\n",
        "accuracy = # ваш код здесь\n",
        "\n",
        "print(\"Test accuracy: %.5f\" % accuracy)\n",
        "assert accuracy > 0.95, \"try training longer\"\n",
        "\n",
        "print('Great job!')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.96265\n",
            "Great job!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXuOnBhYG2yT"
      },
      "source": [
        "\n",
        "### More about pytorch:\n",
        "* Using torch on GPU and multi-GPU - [link](http://pytorch.org/docs/master/notes/cuda.html)\n",
        "* More tutorials on pytorch - [link](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "* Pytorch examples - a repo that implements many cool DL models in pytorch - [link](https://github.com/pytorch/examples)\n",
        "* Practical pytorch - a repo that implements some... other cool DL models... yes, in pytorch - [link](https://github.com/spro/practical-pytorch)\n",
        "* And some more - [link](https://www.reddit.com/r/pytorch/comments/6z0yeo/pytorch_and_pytorch_tricks_for_kaggle/)"
      ]
    }
  ]
}