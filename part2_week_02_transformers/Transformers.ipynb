{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OE7fXh-OSJYF",
    "outputId": "eaa3ba10-4451-44b8-b684-051046aa4ddd"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch\n",
    "!pip -qq install torchtext\n",
    "!pip -qq install numpy\n",
    "!pip install sacremoses==0.0.5\n",
    "!wget -O news.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1hIVVpBqM6VU4n3ERkKq4tFaH4sKN0Hab\"\n",
    "!unzip news.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T03:21:23.176424Z",
     "start_time": "2021-03-02T03:21:21.216482Z"
    },
    "id": "uhvfH55PUJ8K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txWqIO_74A4s"
   },
   "source": [
    "# Abstactive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJ7JQJMO2R7z"
   },
   "source": [
    "Задача - по тексту сгенерировать выдержку из него.\n",
    "\n",
    "Например, попробуем по новостям генерировать заголовки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T03:21:23.280413Z",
     "start_time": "2021-03-02T03:21:21.810Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NzNGUFOcXMUs",
    "outputId": "9117ef66-dd08-4632-a4d3-4532d5090b1d"
   },
   "outputs": [],
   "source": [
    "!shuf -n 10 news.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOVlO5_Qlg5y"
   },
   "source": [
    "Токенизируем их. Будем использовать единый словарь для текста и заголовков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsOvtO0fpCHa"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
    "\n",
    "BOS_TOKEN = '<s>'\n",
    "EOS_TOKEN = '</s>'\n",
    "\n",
    "word_field = Field(tokenize='moses', init_token=BOS_TOKEN, eos_token=EOS_TOKEN, lower=True)\n",
    "fields = [('source', word_field), ('target', word_field)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VO-gix7yoBjg",
    "outputId": "8ede1bde-9579-4fc1-a5bb-d44a82fc9023"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = pd.read_csv('news.csv', delimiter=',')\n",
    "\n",
    "examples = []\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    source_text = word_field.preprocess(row.text)\n",
    "    target_text = word_field.preprocess(row.title)\n",
    "    examples.append(Example.fromlist([source_text, target_text], fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8uCsMEglm6V"
   },
   "source": [
    "Построим датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOBgLAgVTrk1",
    "outputId": "38595aa7-2699-4c7c-a72d-e8fcc3f78608"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(examples, fields)\n",
    "\n",
    "train_dataset, test_dataset = dataset.split(split_ratio=0.85)\n",
    "\n",
    "print('Train size =', len(train_dataset))\n",
    "print('Test size =', len(test_dataset))\n",
    "\n",
    "word_field.build_vocab(train_dataset, min_freq=7)\n",
    "print('Vocab size =', len(word_field.vocab))\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_dataset, test_dataset), batch_sizes=(16, 32), shuffle=True, device=DEVICE, sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHQGgoit2ljv"
   },
   "source": [
    "## Seq2seq for Abstractive Summarization\n",
    "\n",
    "Вообще задача не сильно отличается от машинного перевода:\n",
    "\n",
    "![](https://image.ibb.co/jAf3S0/2018-11-20-9-42-17.png)\n",
    "*From [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/pdf/1704.04368.pdf)*\n",
    "\n",
    "Тут на каждом шаге декодер подглядывает на все токены - точнее, их эмбеддинги после BiRNN.\n",
    "\n",
    "Возникает вопрос - а зачем вообще RNN, если потом все равно будем смотреть на всё."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FYJe2CA8GcY"
   },
   "source": [
    "# Transformer\n",
    "\n",
    "Из этой идеи - отказ от RNN - и получился Transformer.\n",
    "\n",
    "![](https://hsto.org/webt/59/f0/44/59f04410c0e56192990801.png)  \n",
    "*From Attention is all you need*\n",
    "\n",
    "Как в случае с RNN мы на каждом шаге применяем одну и ту же операцию (ячейку LSTM) к текущему входу, так и здесь - только теперь связей между timestamp'ами нет и можно обрабатывать их почти параллельно.\n",
    "\n",
    "*Код дальше очень сильно опирается на шикарную статью [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "env8rDS_dM86"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "Начнем с энкодера:\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm.png)  \n",
    "*From [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)*\n",
    "\n",
    "Он представляет из себя последовательность одинаковых блоков с self-attention + полносвязными слоями.\n",
    "\n",
    "Можно представить, что это - ячейка LSTM: она тоже применяется к каждому входу с одинаковыми весами. Разница основная в отсутствии рекуррентных связей: за счет этого энкодер может применяться одновременно ко всем входам батча."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXjWcnpCJY92"
   },
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Нужно как-то кодировать информацию о том, в каком месте в предложении стоит токен. Чуваки предложили делать так:\n",
    "$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "где $(pos, i)$ - позиция в предложении и индекс в скрытом векторе размерности до $d_{model}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T03:21:27.447149Z",
     "start_time": "2021-03-02T03:21:27.439139Z"
    },
    "id": "zI2rMiZhJcKX"
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-02T03:21:27.653Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "XEL9VppyKCBz",
    "outputId": "84c42bbd-260b-47c5-8647-02d1c91b2b07"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe(torch.zeros(1, 100, 20))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7OeqTcOdgud"
   },
   "source": [
    "В итоге эмбеддинги токена получается как сумма обычного эмбеддинга и эмбеддинга позиции:  \n",
    "![](http://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png)  \n",
    "*From [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rvG4ZQ_icKH"
   },
   "source": [
    "### Residual Connection\n",
    "\n",
    "Разберем блок энкодера - повторяющейся N раз комбинации операций на первом рисунке.\n",
    "\n",
    "Самое простое здесь - residual connection. Вместо к выходу произвольной функции $F$ прибавляется её вход\n",
    "$$y = F(x) \\quad \\to \\quad y = F(x) + x$$\n",
    "\n",
    "Идея в том, что обычные сети сложно делать слишком глубокими - градиенты затухают. А через этот residual вход $x$ градиентам течь ничего не стоит. В итоге в картинках благодаря таким блокам получилось настакать дофига слоев и улучшить качество (см. ResNet).\n",
    "\n",
    "Ничего не мешает нам поступить также."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QLCqNeEjKpD"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self._norm = LayerNorm(size)\n",
    "        self._dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, sublayer):\n",
    "        return inputs + self._dropout(sublayer(self._norm(inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHUqCsifmd5e"
   },
   "source": [
    "### Layer Norm\n",
    "\n",
    "Дополнительно применяется нормализация LayerNorm. \n",
    "\n",
    "**Batch normalization**  \n",
    "Мы вообще не разбирали, но BatchNorm работает так:\n",
    "$$\\mu_j = \\frac{1}{m}\\sum_{i=1}^{m}x_{ij} \\\\    \\sigma_j^2 = \\frac{1}{m}\\sum_{i=1}^{m}(x_{ij} - \\mu_j)^2 \\\\    \\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}}$$\n",
    "$$y_{ij} = \\gamma \\ \\hat{x}_{ij} + \\beta$$\n",
    "\n",
    "На каждом батче эти $\\mu$ и $\\sigma$ пересчитываются, обновляя статистики. На инференсе используются накопленные статистики.\n",
    "\n",
    "Основной его недостаток - он плохо работает с рекуррентными сетями. Чтобы побороть это придумали:\n",
    "\n",
    "**Layer normalization**  \n",
    "А сейчас мы будем пользоваться немного другими формулами:\n",
    "$$\\mu_i = \\frac{1}{m}\\sum_{j=1}^{m}x_{ij} \\\\    \\sigma_i^2 = \\frac{1}{m}\\sum_{j=1}^{m}(x_{ij} - \\mu_i)^2 \\\\    \\hat{x}_{ij} = \\frac{x_{ij} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$$\n",
    "$$y_{ij} = \\gamma \\ \\hat{x}_{ij} + \\beta$$\n",
    "\n",
    "Разницу с ходу не видно, но она есть:\n",
    "![](https://image.ibb.co/hjtuX0/layernorm.png)  \n",
    "*From [Weight Normalization and Layer Normalization Explained ](http://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/)*\n",
    "\n",
    "Если в BatchNorm статистики считаются для каждой фичи усреднением по батчу, то теперь - для каждого входа усредением по фичам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5XLZZ3zrK24"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._gamma = nn.Parameter(torch.ones(features))\n",
    "        self._beta = nn.Parameter(torch.zeros(features))\n",
    "        self._eps = eps\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # PASS CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrWvtymp2G8o"
   },
   "source": [
    "### Attention\n",
    "\n",
    "Весь Transformer опирается на идею self-attention. Выглядит это так:\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self-attention_visualization.png)  \n",
    "*From [Tensor2Tensor Tutorial](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)*\n",
    "\n",
    "Эмбеддинг слова *it* строится как комбинация всех эмбеддингов предложения.\n",
    "\n",
    "В статье придумали делать такой аттеншен:\n",
    "\n",
    "$$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Это примерно как dot-attention на прошлом занятии: запрос (**Q**uery) умножается на ключи (**K**ey) скалярно, затем берется софтмакс - получаются оценки того, насколько интересны разные таймстемпы из значений (**V**alue). \n",
    "\n",
    "Например, $\\mathrm{emb}(\\text{it}) = \\mathrm{Attention}(\\text{it}, \\ldots\\text{because it was too tired}, \\ldots\\text{because it was too tired})$.\n",
    "\n",
    "Только теперь ещё с параметром $\\frac{1}{\\sqrt{d_k}}$, где $d_k$ - это размерность ключа. Утверждается, это работает лучше при больших размерностях ключа $d_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ApuVJZn5i4R"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        # PASS CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZ-xQbgM6MNl"
   },
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "![](https://hsto.org/webt/59/f0/44/59f0440f1109b864893781.png)\n",
    "\n",
    "Важная идея, почему attention (и, главное, self-attention) заработал - использование нескольких голов (multi-head).\n",
    "\n",
    "Вообще, когда мы делаем attention - мы определяем похожесть ключа и запроса. Многоголовость помогает (должна) определять эту похожесть по разным критериям - синтаксически, семантически и т.д.\n",
    "\n",
    "Например, на картинке используется две головы и одна голова смотрит на *the animal* при генерации *it*, вторая - на *tired*:\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png)  \n",
    "*From [Tensor2Tensor Tutorial](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)*\n",
    "\n",
    "Применяется это таким образом:\n",
    "\n",
    "$$\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ...,\n",
    "\\mathrm{head_h})W^O    \\\\\n",
    "    \\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$\n",
    "    \n",
    "где $W^Q_i \\in \\mathbb{R}^{d_{model} \\times d_k}, W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}, W^V_i \\in \\mathbb{R}^{d_{model} \\times d_v}, W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$.\n",
    "\n",
    "В оригинальной статье использовали $h=8$, $d_k=d_v=d_{\\text{model}}/h=64$.\n",
    "\n",
    "Процесс применения такой:\n",
    "![](http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)  \n",
    "*From Illustrated Transformer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rg-CxvPDAJPP"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, heads_count, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % heads_count == 0\n",
    "\n",
    "        self._d_k = d_model // heads_count\n",
    "        self._heads_count = heads_count\n",
    "        self._attention = ScaledDotProductAttention(dropout_rate)\n",
    "        self._attn_probs = None\n",
    "        \n",
    "        self._w_q = nn.Linear(d_model, d_model)\n",
    "        self._w_k = nn.Linear(d_model, d_model)\n",
    "        self._w_v = nn.Linear(d_model, d_model)\n",
    "        self._w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # PASS CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOKwneaKGaJi"
   },
   "source": [
    "### Position-wise Feed-Forward Networks\n",
    "\n",
    "Линейный блок в энкодере выглядит так:\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh1UVkAUGiwh"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(inputs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dZoU1JIt6QP"
   },
   "source": [
    "### Encoder block\n",
    "\n",
    "Соберем все в блок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nh7wQL65sBmk"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._self_attn = self_attn\n",
    "        self._feed_forward = feed_forward\n",
    "        self._self_attention_block = ResidualBlock(size, dropout_rate)\n",
    "        self._feed_forward_block = ResidualBlock(size, dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        outputs = self._self_attention_block(inputs, lambda inputs: self._self_attn(inputs, inputs, inputs, mask))\n",
    "        return self._feed_forward_block(outputs, self._feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8ndCRZLl4ZZ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, d_model),\n",
    "            PositionalEncoding(d_model, dropout_rate)\n",
    "        )\n",
    "        \n",
    "        block = lambda: EncoderBlock(\n",
    "            size=d_model, \n",
    "            self_attn=MultiHeadedAttention(heads_count, d_model, dropout_rate), \n",
    "            feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout_rate),\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self._blocks = nn.ModuleList([block() for _ in range(blocks_count)])\n",
    "        self._norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, inputs, mask):\n",
    "        inputs = self._emb(inputs)\n",
    "\n",
    "        for block in self._blocks:\n",
    "            inputs = block(inputs, mask)\n",
    "            \n",
    "        return self._norm(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pphRcbTvqnq"
   },
   "source": [
    "## Decoder\n",
    "\n",
    "![](https://hsto.org/webt/59/f0/44/59f0440f7d88f805415140.png)\n",
    "\n",
    "Блок декодера (серая часть) состоит уже из трех частей:\n",
    "1. Сперва - тот же self-attention, что и в энкодере\n",
    "2. Затем - стандартный attention на выходы из энкодера + текущее состояние декодера (такой же был в seq2seq with attention)\n",
    "3. Наконец - feed-forward блок\n",
    "\n",
    "Всё это, конечно, с residual связями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LTWjKUXx2LP"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, encoder_attn, feed_forward, dropout_rate):\n",
    "        super().__init__()\n",
    "                \n",
    "        self._self_attn = self_attn\n",
    "        self._encoder_attn = encoder_attn\n",
    "        self._feed_forward = feed_forward\n",
    "        self._self_attention_block = ResidualBlock(size, dropout_rate)\n",
    "        self._attention_block = ResidualBlock(size, dropout_rate)\n",
    "        self._feed_forward_block = ResidualBlock(size, dropout_rate)\n",
    " \n",
    "    def forward(self, inputs, encoder_output, source_mask, target_mask):\n",
    "        outputs = self._self_attention_block(\n",
    "            inputs, lambda inputs: self._self_attn(inputs, inputs, inputs, target_mask)\n",
    "        )\n",
    "        outputs = self._attention_block(\n",
    "            outputs, lambda inputs: self._encoder_attn(inputs, encoder_output, encoder_output, source_mask)\n",
    "        )\n",
    "        return self._feed_forward_block(outputs, self._feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Un0AOmdqLPp_"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, d_model),\n",
    "            PositionalEncoding(d_model, dropout_rate)\n",
    "        )\n",
    "        \n",
    "        block = lambda: DecoderLayer(\n",
    "            size=d_model, \n",
    "            self_attn=MultiHeadedAttention(heads_count, d_model, dropout_rate),\n",
    "            encoder_attn=MultiHeadedAttention(heads_count, d_model, dropout_rate),\n",
    "            feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout_rate),\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self._blocks = nn.ModuleList([block() for _ in range(blocks_count)])\n",
    "        self._norm = LayerNorm(d_model)\n",
    "        self._out_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, encoder_output, source_mask, target_mask):\n",
    "        inputs = self._emb(inputs)\n",
    "        for block in self._blocks:\n",
    "            inputs = block(inputs, encoder_output, source_mask, target_mask)\n",
    "        return self._out_layer(self._norm(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbEkHtwWz0Yh"
   },
   "source": [
    "В декодере нужно аттентиться только на предыдущие токены - сгенерируем маску для этого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbVkZSRq0cD5"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    mask = torch.ones(size, size, device=DEVICE).triu_()\n",
    "    return mask.unsqueeze(0) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "_8X_fqeL0jeW",
    "outputId": "45f6368f-a8c9-4d6d-a291-0d96e8ff9231"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7UEsntVj9lb"
   },
   "source": [
    "## Полная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niG-PnInlF1I"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, d_model=256, d_ff=1024, \n",
    "                 blocks_count=4, heads_count=8, dropout_rate=0.1):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.encoder = Encoder(source_vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate)\n",
    "        self.decoder = Decoder(target_vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate)\n",
    "        self.generator = Generator(d_model, target_vocab_size)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    def forward(self, source_inputs, target_inputs, source_mask, target_mask):\n",
    "        encoder_output = self.encoder(source_inputs, source_mask)\n",
    "        return self.decoder(target_inputs, encoder_output, source_mask, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmucSaUOjlmh"
   },
   "outputs": [],
   "source": [
    "def make_mask(source_inputs, target_inputs, pad_idx):\n",
    "    source_mask = (source_inputs != pad_idx).unsqueeze(-2)\n",
    "    target_mask = (target_inputs != pad_idx).unsqueeze(-2)\n",
    "    target_mask = target_mask & subsequent_mask(target_inputs.size(-1)).type_as(target_mask)\n",
    "    return source_mask, target_mask\n",
    "\n",
    "\n",
    "def convert_batch(batch, pad_idx=1):\n",
    "    source_inputs, target_inputs = batch.source.transpose(0, 1), batch.target.transpose(0, 1)\n",
    "    source_mask, target_mask = make_mask(source_inputs, target_inputs, pad_idx)\n",
    "    \n",
    "    return source_inputs, target_inputs, source_mask, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iWSl6m6jfbl"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_qVuSL8QJg4"
   },
   "outputs": [],
   "source": [
    "model = EncoderDecoder(source_vocab_size=len(word_field.vocab), \n",
    "                  target_vocab_size=len(word_field.vocab)).to(DEVICE)\n",
    "\n",
    "out = model(*convert_batch(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vX6PrVksnaq7"
   },
   "source": [
    "## Оптимизатор\n",
    "\n",
    "Тоже очень важно в данной модели - использовать правильный оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMhopCgTnh-w"
   },
   "outputs": [],
   "source": [
    "class NoamOpt(object):\n",
    "    def __init__(self, model_size, factor=2, warmup=4000, optimizer=None):\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.optimizer = optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuYc21J5oIdb"
   },
   "source": [
    "Идея в том, чтобы повышать learning rate в течении первых warmup шагов линейно, а затем понижать его по сложной формуле:\n",
    "\n",
    "$$\n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot\n",
    "  \\min({step\\_num}^{-0.5},\n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "kMBL261hoA58",
    "outputId": "d8fa0473-a9ab-47af-87f0-5929a1f6cc2c"
   },
   "outputs": [],
   "source": [
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W71i85Q4pdOS"
   },
   "source": [
    "## Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E2JxfRuphch"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                source_inputs, target_inputs, source_mask, target_mask = convert_batch(batch)                                \n",
    "                logits = model.forward(source_inputs, target_inputs[:, :-1], source_mask, target_mask[:, :-1, :-1])\n",
    "                \n",
    "                logits = logits.contiguous().view(-1, logits.shape[-1])\n",
    "                target = target_inputs[:, 1:].contiguous().view(-1)\n",
    "                loss = criterion(logits, target)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "            progress_bar.refresh()\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, val_iter=None):\n",
    "    best_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5X2kYDU_rCjP"
   },
   "outputs": [],
   "source": [
    "model = FullModel(source_vocab_size=len(word_field.vocab), target_vocab_size=len(word_field.vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = word_field.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(DEVICE)\n",
    "\n",
    "optimizer = NoamOpt(model.d_model)\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRudmDewoIPJ"
   },
   "source": [
    "## Жадный перевод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CJKMwKGoGBT"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    # PASS CODE\n",
    "\n",
    "model.eval()\n",
    "src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]])).to(DEVICE)\n",
    "src_mask = Variable(torch.ones(1, 1, 10)).long().to(DEVICE)\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRMFkzDV-wI9"
   },
   "source": [
    "**Задание** Добавьте генератор для модели.\n",
    "\n",
    "**Задание** Добавьте оценку для модели с помощью ROUGE metric (например, из пакета https://pypi.org/project/pyrouge/0.1.3/)\n",
    "\n",
    "**Задание** Добавьте визуализацию механизма attention (можно подсмотреть в коде по ссылкам)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij1GiNx54Ke5"
   },
   "source": [
    "## Улучшения модели\n",
    "\n",
    "**Задание** Попробовать расшарить матрицы эмбеддингов - их тут три (входные в энкодер и декодер + выход декодера).\n",
    "\n",
    "**Задание** Замените лосс на LabelSmoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4-3pYqVJIKA"
   },
   "source": [
    "# Дополнительные материалы\n",
    "\n",
    "## Статьи\n",
    "Attention Is All You Need, 2017 [[pdf]](https://arxiv.org/pdf/1706.03762.pdf)  \n",
    "Get To The Point: Summarization with Pointer-Generator Networks, 2017 [[pdf]](https://arxiv.org/pdf/1704.04368.pdf)  \n",
    "Universal Transformers, 2018 [[arxiv]](https://arxiv.org/abs/1807.03819)\n",
    "\n",
    "## Блоги\n",
    "[Transformer — новая архитектура нейросетей для работы с последовательностями](https://habr.com/post/341240/)  \n",
    "[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)  \n",
    "[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)  \n",
    "[Weighted Tranformer](https://einstein.ai/research/blog/weighted-transformer)  \n",
    "[Your tldr by an ai: a deep reinforced model for abstractive summarization](https://einstein.ai/research/blog/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"Week 11 - Transformers.ipynb\"",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
