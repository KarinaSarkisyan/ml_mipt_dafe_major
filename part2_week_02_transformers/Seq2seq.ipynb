{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OE7fXh-OSJYF",
    "outputId": "dfece962-0059-4254-82c2-180d67b39a71"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch\n",
    "!pip -qq install torchtext\n",
    "!pip -qq install torchvision\n",
    "!pip -qq install spacy\n",
    "!python -m spacy download en\n",
    "!pip install sacremoses\n",
    "!pip install subword_nmt\n",
    "!wget -qq http://www.manythings.org/anki/rus-eng.zip \n",
    "!unzip rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhvfH55PUJ8K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txWqIO_74A4s"
   },
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRiSIqhQcP2W"
   },
   "source": [
    "Мы уже несколько раз смотрели на эту картинку:\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "В POS tagging мы (ну, некоторые точно) использовали важную идею: сначала некоторой функцией над символами строился эмбеддинг для слова (например, many to one rnn'кой на картинке). Потом другая rnn'ка строила эмбеддинги слов с учетом их контекста. А дальше это всё классифицируется логистической регрессией.\n",
    "\n",
    "Тут важно то, что мы обучаем encoder для построения эмбеддингов end2end - прямо в составе сети (это основное отличие нейросетей от классических подходов - в умении делать end2end).\n",
    "\n",
    "Другое, что мы делали - это языковые модели. Вот, типа такого:\n",
    "![](https://hsto.org/web/dc1/7c2/c4e/dc17c2c4e9ac434eb5346ada2c412c9a.png)\n",
    "\n",
    "Обратите внимание на красную стрелку - она показывает передачу скрытого состояния, которое отвечает за память сети.\n",
    "\n",
    "А теперь совместим эти две идеи:\n",
    "\n",
    "![](https://raw.githubusercontent.com/tensorflow/nmt/master/nmt/g3doc/img/seq2seq.jpg)  \n",
    "*From [tensorflow/nmt](https://github.com/tensorflow/nmt)*\n",
    "\n",
    "Выглядит все почти как языковая модель, но в синей части предсказания не делаются, используется только последнее скрытое состояние.\n",
    "\n",
    "Синяя часть сети называется энкодер, она строит эмбеддинг последовательности. Красная часть - декодер, работает как обычная языковая модель, но учитывает результат работы энкодера.\n",
    "\n",
    "В итоге, энкодер учится эффективно извлекать смысл из последовательности слов, а декодер должен строить по ним новую последовательность. Это может быть последовательностью слов перевода, или последовательностью слов в ответе чат бота, или еще чем-то в зависимости от вашей испорченности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyNkst1XkYN6"
   },
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRLNIzJkkqkM"
   },
   "source": [
    "Начнем с чтения данных. Возьмем их у anki, поэтому они немного специфичны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U2vjzBVqZF0b",
    "outputId": "f07e1922-4ab2-4302-a705-d510e882a0a1"
   },
   "outputs": [],
   "source": [
    "!shuf -n 10 rus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOVlO5_Qlg5y"
   },
   "source": [
    "Токенизируем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsOvtO0fpCHa"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
    "\n",
    "BOS_TOKEN = '<s>'\n",
    "EOS_TOKEN = '</s>'\n",
    "\n",
    "source_field = Field(tokenize='spacy', init_token=None, eos_token=EOS_TOKEN)\n",
    "target_field = Field(tokenize='moses', init_token=BOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "fields = [('source', source_field), ('target', target_field)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0qA15-tcudjw",
    "outputId": "d2563e61-f905-4567-f668-d6bc94a36fc0"
   },
   "outputs": [],
   "source": [
    "source_field.preprocess(\"It's surprising that you haven't heard anything about her wedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HguPFHc5sjcD",
    "outputId": "f0c71e7b-58e8-45fa-862a-ec91e0c1282a"
   },
   "outputs": [],
   "source": [
    "target_field.preprocess('Удивительно, что ты ничего не слышал о её свадьбе.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VO-gix7yoBjg",
    "outputId": "3d3d7c94-b3f5-4af5-8c1c-d4690db206b7"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "MAX_TOKENS_COUNT = 16\n",
    "SUBSET_SIZE = .3\n",
    "\n",
    "examples = []\n",
    "with open('rus.txt') as f:\n",
    "    for line in tqdm(f):\n",
    "        source_text, target_text, _ = line.split('\\t')\n",
    "        source_text = source_field.preprocess(source_text)\n",
    "        target_text = target_field.preprocess(target_text)\n",
    "        if len(source_text) <= MAX_TOKENS_COUNT and len(target_text) <= MAX_TOKENS_COUNT:\n",
    "            if np.random.rand() < SUBSET_SIZE:\n",
    "                examples.append(Example.fromlist([source_text, target_text], fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8uCsMEglm6V"
   },
   "source": [
    "Построим датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOBgLAgVTrk1",
    "outputId": "b28c2ecb-d7d8-43fa-eb3f-b719d8c747ca"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(examples, fields)\n",
    "\n",
    "train_dataset, test_dataset = dataset.split(split_ratio=0.85)\n",
    "\n",
    "print('Train size =', len(train_dataset))\n",
    "print('Test size =', len(test_dataset))\n",
    "\n",
    "source_field.build_vocab(train_dataset, min_freq=3)\n",
    "print('Source vocab size =', len(source_field.vocab))\n",
    "\n",
    "target_field.build_vocab(train_dataset, min_freq=3)\n",
    "print('Target vocab size =', len(target_field.vocab))\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train_dataset, test_dataset), batch_sizes=(32, 256), shuffle=True, device=DEVICE, sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5sMz-hfBvCl",
    "outputId": "28c31b32-1866-473f-9d05-e95472e2028b"
   },
   "outputs": [],
   "source": [
    "source_field.process([source_field.preprocess(\"It's surprising that you haven't heard anything about her wedding.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1yFzRg2xAjl",
    "outputId": "79500e5f-97ce-47d8-9f1c-fa2e9a8d2536"
   },
   "outputs": [],
   "source": [
    "source_field.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19X2G_QpxJEx",
    "outputId": "ebc5cc46-5f4f-4631-c92a-ac16e6c3a759"
   },
   "outputs": [],
   "source": [
    "target_field.vocab.itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FYJe2CA8GcY"
   },
   "source": [
    "## Seq2seq модель\n",
    "\n",
    "Пора писать простой seq2seq. Разобьем модель на несколько модулей - Encoder, Decoder и их объединение. \n",
    "\n",
    "Encoder должен быть подобен символьной сеточке в POS tagging'е: эмбеддить токены и запускать rnn'ку (в данном случае будем пользоваться GRU) и отдавать последнее скрытое состояние.\n",
    "\n",
    "Decoder почти такой же, только еще и предсказывает токены на каждом своем шаге.\n",
    "\n",
    "**Задание** Реализовать модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySJ4tUAqvFvB"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8ndCRZLl4ZZ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, rnn_hidden_dim=256, num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.GRU(input_size=emb_dim, hidden_size=rnn_hidden_dim, \n",
    "                           num_layers=num_layers, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        \"\"\"\n",
    "        input: LongTensor with shape (encoder_seq_len, batch_size)\n",
    "        hidden: FloatTensor with shape (1, batch_size, rnn_hidden_dim)\n",
    "        \"\"\"\n",
    "        # PASS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Un0AOmdqLPp_"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, rnn_hidden_dim=256, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self._rnn = nn.GRU(input_size=emb_dim, hidden_size=rnn_hidden_dim, num_layers=num_layers)\n",
    "        self._out = nn.Linear(rnn_hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, encoder_output, hidden=None):\n",
    "        \"\"\"\n",
    "        input: LongTensor with shape (decoder_seq_len, batch_size)\n",
    "        encoder_output: FloatTensor with shape (encoder_seq_len, batch_size, rnn_hidden_dim)\n",
    "        encoder_mask: ByteTensor with shape (encoder_seq_len, batch_size) (ones in positions of <pad> tokens, zeros everywhere else)\n",
    "        hidden: FloatTensor with shape (1, batch_size, rnn_hidden_dim)\n",
    "        \"\"\"\n",
    "        # PASS CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9nsO1HCmgn3"
   },
   "source": [
    "Модель перевода будет просто сперва вызывать Encoder, а потом передавать его скрытое состояние декодеру в качестве начального."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLIGjPOiO7X9"
   },
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, emb_dim=128, \n",
    "                 rnn_hidden_dim=256, num_layers=1, bidirectional_encoder=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(source_vocab_size, emb_dim, rnn_hidden_dim, num_layers, bidirectional_encoder)\n",
    "        self.decoder = Decoder(target_vocab_size, emb_dim, rnn_hidden_dim, num_layers)\n",
    "        \n",
    "    def forward(self, source_inputs, target_inputs=None):\n",
    "        encoder_output, encoder_hidden = self.encoder(source_inputs)\n",
    "        \n",
    "        return self.decoder(target_inputs, encoder_hidden, encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_qVuSL8QJg4",
    "outputId": "d4952b06-1205-4dce-833e-0a312a8f0189"
   },
   "outputs": [],
   "source": [
    "model = TranslationModel(source_vocab_size=len(source_field.vocab), \n",
    "                         target_vocab_size=len(target_field.vocab)).to(DEVICE)\n",
    "\n",
    "out, _ = model(batch.source.to(DEVICE), batch.target.to(DEVICE))\n",
    "print(out.shape, batch.source.shape, batch.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz4Ckgh1mwm4"
   },
   "source": [
    "Реализуем простой перевод - жадный. На каждом шаге будем выдавать самый вероятный из предсказываемых токенов:\n",
    "\n",
    "![](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/greedy_dec.jpg)  \n",
    "*From [tensorflow/nmt](https://github.com/tensorflow/nmt)*\n",
    "\n",
    "**Задание** Реализовать функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9gmcOC9DwiS"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, source_text, source_field, target_field):\n",
    "    bos_index = target_field.vocab.stoi[BOS_TOKEN]\n",
    "    eos_index = target_field.vocab.stoi[EOS_TOKEN]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = []\n",
    "\n",
    "        # PASS CODE\n",
    "        \n",
    "        return ' '.join(target_field.vocab.itos[ind] for ind in result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "mM58pAd6FBml",
    "outputId": "ef4048af-a359-4a64-fcc0-ff4fe1a40c82"
   },
   "outputs": [],
   "source": [
    "greedy_decode(model, \"Do you believe?\", source_field, target_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-ha7DI_ngAO"
   },
   "source": [
    "Нужно как-то оценивать модель.\n",
    "\n",
    "Обычно для этого используется [BLEU скор](https://en.wikipedia.org/wiki/BLEU) - что-то вроде точности угадывания n-gram из правильного (референсного) перевода.\n",
    "\n",
    "**Задание** Реализовать функцию оценки: для батчей из `iterator` предсказать их переводы, обрезать по `</s>` и сложить правильные варианты и предсказанные в `refs` и `hyps` соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjYA3eohGlOA"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate_model(model, iterator):\n",
    "    model.eval()\n",
    "    refs, hyps = [], []\n",
    "    bos_index = iterator.dataset.fields['target'].vocab.stoi[BOS_TOKEN]\n",
    "    eos_index = iterator.dataset.fields['target'].vocab.stoi[EOS_TOKEN]\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            encoder_output, encoder_hidden = model.encoder(batch.source)\n",
    "            \n",
    "            # PASS CODE\n",
    "            \n",
    "    return corpus_bleu([[ref] for ref in refs], hyps) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E2JxfRuphch"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.get_lock().locks = []\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data_iter, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = len(data_iter)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):                \n",
    "                logits, _ = model(batch.source, batch.target)\n",
    "                \n",
    "                target = torch.cat((batch.target[1:], batch.target.new_ones((1, batch.target.shape[1]))))\n",
    "                loss = criterion(logits.view(-1, logits.shape[-1]), target.view(-1))\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
    "                                                                                         math.exp(loss.item())))\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
    "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
    "            )\n",
    "            progress_bar.refresh()\n",
    "\n",
    "    return epoch_loss / batches_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_iter, epochs_count=1, val_iter=None):\n",
    "    best_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss = do_epoch(model, criterion, train_iter, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_iter is None:\n",
    "            val_loss = do_epoch(model, criterion, val_iter, None, name_prefix + '  Val:')\n",
    "            print('\\nVal BLEU = {:.2f}'.format(evaluate_model(model, val_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "5X2kYDU_rCjP",
    "outputId": "60b69a4a-4edc-4018-c9e3-283b053692dd"
   },
   "outputs": [],
   "source": [
    "model = TranslationModel(source_vocab_size=len(source_field.vocab), target_vocab_size=len(target_field.vocab)).to(DEVICE)\n",
    "\n",
    "pad_idx = target_field.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_iter, epochs_count=30, val_iter=test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sJ4GilOusNCP",
    "outputId": "dcf630cd-531d-4696-ea60-acf4951707f4"
   },
   "outputs": [],
   "source": [
    "greedy_decode(model, \"Do you want to believe ?\", source_field, target_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "es0D28m5jdNF"
   },
   "source": [
    "## Scheduled Sampling\n",
    "\n",
    "До сих пор мы тренировали перевод, используя так называемый *teacher forcing*: в качестве выхода на предыдущем шаге декодер принимал всегда правильный токен. Проблема такого подхода - во время инференса правильный токен, скорее всего, не выберется хотя бы на каком-то шаге. Получится, что сеть училась на хороших входах, использоваться будет на плохом - это легко может всё поломать.\n",
    "\n",
    "Альтернативный подход - прямо во время обучения сэмплировать токен с текущего шага и передавать его на следующий.\n",
    "\n",
    "Такой подход не слишком-то обоснован математически (градиенты не прокидываются через сэмплирование), но его интересно реализовать и он зачастую улучшает качество.\n",
    "\n",
    "**Задание** Обновите `Decoder`: замените вызов rnn'ки над последовательностью на цикл. На каждом шаге вероятностью `p` передавайте в качестве предыдущего выхода декодеру правильный вход, а иначе - argmax от предыдущего выхода (цикл должен быть похожим на те, которые есть в `greedy_decode` и `evaluate_model`). При передаче argmax вызывайте `detach`, чтобы градиенты не прокидывались. Все выходы собирайте в список, в конце сделайте `torch.cat`. \n",
    "\n",
    "В результате при вероятности, равной `p=1`, должно получиться как раньше, только медленнее. При обучении можно передавать `p=0.5`, на инференсе - `p=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK4OdgTRQmqp"
   },
   "source": [
    "## Улучшения модели\n",
    "\n",
    "**Задание** Попробуйте повысить качество модели. Попробуйте: \n",
    "- Bidirectional encoder\n",
    "- Dropout\n",
    "- Stack moar layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4-3pYqVJIKA"
   },
   "source": [
    "# Дополнительные материалы\n",
    "\n",
    "## Статьи\n",
    "Sequence to Sequence Learning with Neural Networks, Ilya Sutskever, et al, 2014 [[pdf]](https://arxiv.org/pdf/1409.3215.pdf)  \n",
    "Show and Tell: A Neural Image Caption Generator, Oriol Vinyals et al, 2014 [[arxiv]](https://arxiv.org/abs/1411.4555)  \n",
    "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks, Samy Bengio, et al, 2015 [[arxiv]](https://arxiv.org/abs/1506.03099)  \n",
    "Neural Machine Translation of Rare Words with Subword Units, Rico Sennrich, 2015 [[arxiv]](https://arxiv.org/abs/1508.07909)  \n",
    "Massive Exploration of Neural Machine Translation Architectures, Denny Britz, et al, 2017 [[pdf]](https://arxiv.org/pdf/1703.03906.pdf)\n",
    "\n",
    "## Блоги\n",
    "Neural Machine Translation (seq2seq) Tutorial [tensorflow/nmt](https://github.com/tensorflow/nmt)  \n",
    "[A Word of Caution on Scheduled Sampling for Training RNNs](https://www.inference.vc/scheduled-sampling-for-rnns-scoring-rule-interpretation/)\n",
    "\n",
    "## Видео\n",
    "cs224n, [Machine Translation, Seq2Seq and Attention](https://www.youtube.com/watch?v=IxQtK2SjWWM)\n",
    "\n",
    "## Разное\n",
    "[The Annotated Encoder Decoder](https://bastings.github.io/annotated_encoder_decoder/)  \n",
    "[Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models](http://seq2seq-vis.io)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"Week 09 - Seq2seq.ipynb\"",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
